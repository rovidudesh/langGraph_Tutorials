{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31b18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a669cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY=os.getenv(\"GEMINI_API_KEY\")\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35dfaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ef1f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b063fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\" , google_api_key=GEMINI_API_KEY )\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                            google_api_key=GEMINI_API_KEY , \n",
    "                            temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df3bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3361b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information. Use cases include providing chatbot access to internal company data or generating responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.\n",
      "By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to IBM, \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting.\"\n",
      "Beyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "The term \"retrieval-augmented generation\" (RAG) was first introduced in 2020 by Douwe Kiela, Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and Sebastian Riedel in their research paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, at Meta.\n",
      "\n",
      "Page: Model Context Protocol\n",
      "Summary: The Model Context Protocol (MCP) is an open standard developed by the artificial intelligence company Anthropic for enabling large language model (LLM) applications to interact with external tools, systems, and data sources. Designed to standardize context exchange between AI assistants and software environments, MCP provides a model-agnostic interface for reading files, executing functions, and handling contextual prompts. It was officially announced and open-sourced by Anthropic in November 2024, with subsequent adoption by major AI providers including OpenAI and Google DeepMind.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "print(tool.run({\"query\": \"langchain\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe53f714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=ktjJAxaX8rc&pp=ygUMc3Vubnkgc2F2aXRh', 'https://www.youtube.com/watch?v=rnPtpNTLuT8&pp=ygUMc3Vubnkgc2F2aXRh0gcJCYQJAYcqIYzv']\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "tool2 = YouTubeSearchTool()\n",
    "tool2.run(\"sunny savita\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a4de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b51e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_length.invoke(\"abc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a808e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools \n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fe13c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool=load_tools([\"wikipedia\"],llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2549649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LapMaster\\AppData\\Local\\Temp\\ipykernel_17256\\1791468372.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent=initialize_agent(tool,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent=initialize_agent(tool,llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f77b6038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LapMaster\\AppData\\Local\\Temp\\ipykernel_17256\\1615792405.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"what is llama and who create this llm model?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find out what LLaMA is and who created it. I can use Wikipedia to search for \"LLaMA\" and then analyze the search results.\n",
      "Action: wikipedia\n",
      "Action Input: LLaMA\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Llama (language model)\n",
      "Summary: Llama (Large Language Model Meta AI, formerly stylized as LLaMA) is a family of large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 4, released in April 2025.\n",
      "Llama models come in different sizes, ranging from 1 billion to 2 trillion parameters. Initially only a foundation model, starting with Llama 2, Meta AI released instruction fine-tuned versions alongside foundation models.\n",
      "Model weights for the first version of Llama were only available to researchers on a case-by-case basis, under a non-commercial license. Unauthorized copies of the first model were shared via BitTorrent. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.\n",
      "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n",
      "\n",
      "\n",
      "\n",
      "Page: Llama\n",
      "Summary: The llama (; Spanish pronunciation: [ˈʎama] or [ˈʝama]) (Lama glama) is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.\n",
      "Llamas are social animals and live with others as a herd. Their wool is soft and contains only a small amount of lanolin. Llamas can learn simple tasks after a few repetitions. When using a pack, they can carry about 25 to 30% of their body weight for 8 to 13 km (5–8 miles). The name llama (also historically spelled \"lama\" or \"glama\") was adopted by European settlers from native Peruvians.\n",
      "The ancestors of llamas are thought to have originated on the Great Plains of North America about 40 million years ago and subsequently migrated to South America about three million years ago during the Great American Interchange. By the end of the last ice age (10,000–12,000 years ago), camelids were extinct in North America. As of 2007, there were over seven million llamas and alpacas in South America. Some were imported to the United States and Canada late in the 20th century; their descendants now number more than 158,000 llamas and 100,000 alpacas.\n",
      "In Aymara mythology, llamas are important beings. The Heavenly Llama is said to drink water from the ocean and urinates as it rains. According to Aymara eschatology, llamas will return to the water springs and ponds where they come from at the end of time.\n",
      "\n",
      "\n",
      "\n",
      "Page: Generative artificial intelligence\n",
      "Summary: Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \n",
      "Generative AI tools have become more common since an \"AI boom\" in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Technology companies developing generative AI include OpenAI, Anthropic, Microsoft, Google, and Baidu.\n",
      "Generative AI has raised many ethical questions. It can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to the mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on and emulate copyrighted works of art. \n",
      "Generative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. \n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have the information I need. LLaMA is a family of large language models (LLMs) released by Meta AI.\n",
      "Action: None\n",
      "Final Answer: LLaMA is a family of large language models released by Meta AI.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLaMA is a family of large language models released by Meta AI.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"what is llama and who create this llm model?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "375fe3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df5a9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db5c87c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Weather in San Francisco, California',\n",
       "  'url': 'https://www.weatherapi.com/',\n",
       "  'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1746106191, 'localtime': '2025-05-01 06:29'}, 'current': {'last_updated_epoch': 1746105300, 'last_updated': '2025-05-01 06:15', 'temp_c': 12.2, 'temp_f': 54.0, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 207, 'wind_dir': 'SSW', 'pressure_mb': 1015.0, 'pressure_in': 29.96, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 75, 'feelslike_c': 11.4, 'feelslike_f': 52.4, 'windchill_c': 7.2, 'windchill_f': 45.0, 'heatindex_c': 9.0, 'heatindex_f': 48.1, 'dewpoint_c': 8.7, 'dewpoint_f': 47.6, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 9.0, 'gust_kph': 14.5}}\",\n",
       "  'score': 0.8528406},\n",
       " {'title': 'San Francisco, CA Weather Forecast - AccuWeather',\n",
       "  'url': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629',\n",
       "  'content': 'Current Weather --------------- 9:15 PM 48°F RealFeel® 49° Clear More Details Wind WNW 3 mph Wind Gusts 5 mph Air Quality Poor\\nSan Francisco Weather Radar & Maps\\n Static Radar Temporarily Unavailable Thank you for your patience as we work to get everything up and running again. Refresh Page\\n Clouds  Temperature\\nHourly Weather\\n10 PM  47°  0%11 PM  46°  0%12 AM  45°  0%1 AM  45°  0%2 AM  44°  0%3 AM  43°  0%4 AM  42°  0%5 AM  42°  0%6 AM  42°  0%7 AM  41°  0%8 AM  42°  0%9 AM  44°  0%',\n",
       "  'score': 0.657561},\n",
       " {'title': 'Current Weather - San Francisco, CA - AccuWeather',\n",
       "  'url': 'https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629',\n",
       "  'content': '55°Hi\\nRealFeel® 58°\\nCool\\nRealFeel Guide\\nCool\\n53° to 62°\\nLight jacket or sweater may be appropriate.\\nLEARN MORE\\nRealFeel Shade™ 53°\\nChilly\\nRealFeel Guide\\nChilly\\n53° to 62°\\nLight jacket or sweater may be appropriate.\\nLEARN MORE\\nLow clouds breaking for some sun and cool\\nMax UV Index5 Moderate\\nWindW 14 mph\\nWind Gusts31 mph\\nProbability of Precipitation6%\\nProbability of Thunderstorms0%\\nPrecipitation0.00 in\\nCloud Cover81%\\nMorning\\nAfternoon\\nNight\\n4/24\\n50°Lo\\nRealFeel® 42°\\nChilly\\nRealFeel Guide\\nChilly [...] Current Weather\\n2:30 PM\\n55°F\\nMostly cloudy\\nRealFeel® 58°\\nCool\\nRealFeel Guide\\nCool\\n53° to 62°\\nLight jacket or sweater may be appropriate.\\nLEARN MORE\\nRealFeel Shade™ 53°\\nCool\\nRealFeel Guide\\nCool\\n53° to 62°\\nLight jacket or sweater may be appropriate.\\nLEARN MORE\\nRealFeel®\\n58°\\nRealFeel Shade™\\n53°\\nMax UV Index\\n3 Moderate\\nWind\\nW 6 mph\\nWind Gusts\\n11 mph\\nHumidity\\n68%\\nIndoor Humidity\\n44% (Ideal Humidity)\\nDew Point\\n45° F\\nPressure\\n↔ 30.04 in\\nCloud Cover\\n76%\\nVisibility\\n9 mi\\nCloud Ceiling\\n5700 ft\\nDay\\n4/24',\n",
       "  'score': 0.6241337},\n",
       " {'title': 'San Francisco - BBC Weather',\n",
       "  'url': 'https://www.bbc.com/weather/5391959',\n",
       "  'content': '01:00\\n,\\nPartly Cloudy\\nPartly Cloudy\\n10° 50°\\n,\\n0%chance of precipitation\\n,\\nWind speed18 mph28 km/h WNW18 mph28 km/hWest North Westerly\\n, More details\\xa0\\nPartly cloudy and a moderate breeze\\nHumidity\\n81%\\nPressure\\n1020 mb\\nVisibility\\nGood\\nTemperature feels like 8°47°\\nPrecipitation is not expected\\nA moderate breeze from the west north west [...] 19:00\\n,\\nPartly Cloudy\\nPartly Cloudy\\n11° 53°\\n,\\n0%chance of precipitation\\n,\\nWind speed23 mph37 km/h WNW23 mph37 km/hWest North Westerly\\n, More details\\xa0\\nPartly cloudy and a fresh breeze\\nHumidity\\n76%\\nPressure\\n1019 mb\\nVisibility\\nGood\\nTemperature feels like 9°49°\\nPrecipitation is not expected\\nA fresh breeze from the west north west [...] 00:00\\nMon\\n,\\nPartly Cloudy\\nPartly Cloudy\\n10° 51°\\n,\\n0%chance of precipitation\\n,\\nWind speed18 mph29 km/h WNW18 mph29 km/hWest North Westerly\\n, More details\\xa0\\nPartly cloudy and a moderate breeze\\nHumidity\\n80%\\nPressure\\n1019 mb\\nVisibility\\nGood\\nTemperature feels like 8°47°\\nPrecipitation is not expected\\nA moderate breeze from the west north west',\n",
       "  'score': 0.6102915},\n",
       " {'title': 'Hourly Weather Forecast for San Francisco, CA',\n",
       "  'url': 'https://weather.com/weather/hourbyhour/l/USCA0987:1:US',\n",
       "  'content': 'Feels Like47°\\n\\n\\nWindNW 4\\xa0mph\\n\\n\\nHumidity89%\\n\\n\\nUV Index0 of 11\\n\\n\\nCloud Cover21%\\n\\n\\nRain Amount0 in\\n\\n\\n1 am\\nMostly Clear\\n47°\\n7%\\nNW 3\\xa0mph\\nMostly Clear\\n\\n\\nFeels Like47°\\n\\n\\nWindNW 3\\xa0mph\\n\\n\\nHumidity90%\\n\\n\\nUV Index0 of 11\\n\\n\\nCloud Cover23%\\n\\n\\nRain Amount0 in\\n\\n\\n2 am\\nClear\\n46°\\n10%\\nNW 2\\xa0mph\\nClear\\n\\n\\nFeels Like46°\\n\\n\\nWindNW 2\\xa0mph\\n\\n\\nHumidity93%\\n\\n\\nUV Index0 of 11\\n\\n\\nCloud Cover17%\\n\\n\\nRain Amount0 in\\n\\n\\n3 am\\nMostly Clear\\n46°\\n12%\\nN 2\\xa0mph\\nMostly Clear\\n\\n\\nFeels Like46°\\n\\n\\nWindN 2\\xa0mph\\n\\n\\nHumidity93%\\n\\n\\nUV Index0 of 11 [...] Cloud Cover20%\\n\\n\\nRain Amount0 in\\n\\n\\n4 am\\nClear\\n46°\\n9%\\nNNE 3\\xa0mph\\nClear\\n\\n\\nFeels Like44°\\n\\n\\nWindNNE 3\\xa0mph\\n\\n\\nHumidity94%\\n\\n\\nUV Index0 of 11\\n\\n\\nCloud Cover19%\\n\\n\\nRain Amount0 in\\n\\n\\n5 am\\nClear\\n45°\\n14%\\nNNE 3\\xa0mph\\nClear\\n\\n\\nFeels Like45°\\n\\n\\nWindNNE 3\\xa0mph\\n\\n\\nHumidity96%\\n\\n\\nUV Index0 of 11\\n\\n\\nCloud Cover18%\\n\\n\\nRain Amount0 in\\n\\n\\n6 am\\nClear\\n45°\\n10%\\nNNE 2\\xa0mph\\nClear\\n\\n\\nFeels Like45°\\n\\n\\nWindNNE 2\\xa0mph\\n\\n\\nHumidity95%\\n\\n\\nUV Index0 of 11\\n\\n\\nCloud Cover18%\\n\\n\\nRain Amount0 in\\n\\n\\n7 am\\nSunny\\n45°\\n8%\\nN 2\\xa0mph\\nSunny\\n\\n\\nFeels Like45° [...] 9 am\\nPartly Cloudy\\n51°\\n6%\\nNW 8\\xa0mph\\nPartly Cloudy\\n\\n\\nFeels Like49°\\n\\n\\nWindNW 8\\xa0mph\\n\\n\\nHumidity71%\\n\\n\\nUV Index1 of 11\\n\\n\\nCloud Cover39%\\n\\n\\nRain Amount0 in\\n\\n\\n10 am\\nPartly Cloudy\\n53°\\n6%\\nNW 8\\xa0mph\\nPartly Cloudy\\n\\n\\nFeels Like50°\\n\\n\\nWindNW 8\\xa0mph\\n\\n\\nHumidity66%\\n\\n\\nUV Index3 of 11\\n\\n\\nCloud Cover35%\\n\\n\\nRain Amount0 in\\n\\n\\n11 am\\nPartly Cloudy\\n54°\\n5%\\nNW 8\\xa0mph\\nPartly Cloudy\\n\\n\\nFeels Like51°\\n\\n\\nWindNW 8\\xa0mph\\n\\n\\nHumidity62%\\n\\n\\nUV Index4 of 11\\n\\n\\nCloud Cover42%\\n\\n\\nRain Amount0 in',\n",
       "  'score': 0.6100127}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.invoke(\"what is the weather in SF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce5f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fd1a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4b17c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87230637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb427f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e071ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI am doing well, thank you for asking. How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hello how are you?',\n",
       " 'output': 'I am doing well, thank you for asking. How can I help you today?'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"hello how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a911eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
